"""
People counting analytics using YOLO, RT-DETR, and BotSort with depth-aware detection and track merging.
Supports image, video, and MOT dataset inputs.
"""

# ======================================
# Imports and Setup
# ======================================
import os
import glob
import cv2
import torch
import time
import numpy as np
import logging
import re
import requests
import tempfile
import shutil  # FIX: Added missing import
import urllib.request  # FIX: Added missing import
from pathlib import Path
from tqdm import tqdm
from typing import Dict, List, Optional
from django.conf import settings
from django.core.files.storage import default_storage
from django.utils import timezone
import mimetypes
from celery import shared_task
from ultralytics import RTDETR
from boxmot import BotSort
import torch.nn.functional as F

# ======================================
# Logger Setup (must be first)
# ======================================
logger = logging.getLogger("dubs_people_counting_comprehensive")

# Hugging Face imports with fallback
try:
    from huggingface_hub import snapshot_download
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    logger.warning("âš ï¸ huggingface_hub not available - HF model downloads will be skipped")

from ..utils import load_yolo_model
from ..convert import convert_to_web_mp4

# Import scipy with fallback
try:
    from scipy.spatial.distance import cosine
    SCIPY_AVAILABLE = True
except ImportError:
    SCIPY_AVAILABLE = False
    logger.warning("âš ï¸ scipy not available - using numpy cosine similarity fallback")

# MiDaS imports with proper error handling
try:
    import torch.hub
    MIDAS_AVAILABLE = True
except ImportError:
    MIDAS_AVAILABLE = False
    logger.warning("âš ï¸ torch.hub not available for MiDaS")

# Import progress logger
try:
    from ..progress_logger import create_progress_logger
except ImportError:
    def create_progress_logger(job_id, total_items, job_type, logger_name=None):
        """Fallback progress logger if module not available."""
        class DummyLogger:
            def __init__(self, job_id, total_items, job_type, logger_name=None):
                self.job_id = job_id
                self.total_items = total_items
                self.job_type = job_type
                self.logger = logging.getLogger(logger_name or job_type)

            def update_progress(self, processed_count, status=None, force_log=False):
                self.logger.info(f"**Job {self.job_id}**: Progress {processed_count}/{self.total_items}")

            def log_completion(self, final_count=None):
                self.logger.info(f"**Job {self.job_id}**: Completed {self.job_type}")

        return DummyLogger(job_id, total_items, job_type, logger_name)
VALID_EXTENSIONS = {'.mp4', '.jpg', '.jpeg', '.png'}
MAX_FILE_SIZE = 500 * 1024 * 1024  # 500MB
MODELS_DIR = Path(__file__).resolve().parent.parent / "models"
# Re-ID model is now downloaded to torch cache directory by the new download_models function
REID_MODEL_PATH = Path.home() / ".cache/torch/checkpoints/osnet_x0_25_msmt17.pt"
YOLO_MODEL_PATH = MODELS_DIR / "yolov12x.pt"  # Updated to match new naming
RTDETR_MODEL_PATH = MODELS_DIR / "rtdetr-l.pt"
MIDAS_WEIGHTS_PATH = MODELS_DIR / "dpt_large_384.pt"  # Updated to match new naming
MIDAS_REPO = 'intel-isl/MiDaS'
MIDAS_MODEL_NAME = 'DPT_Large'
DPT_HF_MODEL = 'Intel/dpt-large'
GLPN_HF_MODEL = 'vinvino02/glpn-nyu'
DEFAULT_OUTPUT_PREFIX = 'dubs_comprehensive_output_'
DEFAULT_OUTPUT_EXT = '.mp4'
DEFAULT_USE_REID = True
DEFAULT_POST_PROCESS = True
YOLO_CONF_CLOSE = 0.35
YOLO_CONF_MEDIUM = 0.40
YOLO_CONF_FAR = 0.45
RTDETR_CONF_CLOSE = 0.30
RTDETR_CONF_MEDIUM = 0.35
RTDETR_CONF_FAR = 0.40
DEFAULT_NMS_IOU_THRESH = 0.4
MIN_LIFETIME_FRAMES = 40

# Define OUTPUT_DIR with fallback
try:
    OUTPUT_DIR = Path(settings.JOB_OUTPUT_DIR)
except AttributeError:
    logger.warning("JOB_OUTPUT_DIR not defined in settings. Using fallback: MEDIA_ROOT/outputs")
    OUTPUT_DIR = Path(settings.MEDIA_ROOT) / 'outputs'
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ======================================
# Model Download Function
# ======================================

def download_models(models_dir=None):
    """
    Ensure all required models are present.
    - Ultralytics YOLO and RT-DETR models
    - MiDaS DPT_Large (torch.hub)
    - DPT/GLPN (Hugging Face Transformers)
    - BotSort Re-ID weights
    Downloads them as needed using the same library mechanisms as in your analytics code.
    """
    # Use MODELS_DIR if no specific directory provided
    if models_dir is None:
        models_dir = MODELS_DIR
    else:
        models_dir = Path(models_dir)

    # FIX: Create models directory with proper permissions handling
    try:
        models_dir.mkdir(parents=True, exist_ok=True)
        logger.info(f"ðŸ“ Models directory: {models_dir}")
    except PermissionError:
        # Fallback to user home directory if system directory fails
        models_dir = Path.home() / ".cache" / "dubs_models"
        models_dir.mkdir(parents=True, exist_ok=True)
        logger.warning(f"âš ï¸ Permission denied for models directory. Using fallback: {models_dir}")

    # 1. Ultralytics YOLO & RT-DETR (auto-download via library)
    try:
        from ultralytics import YOLO, RTDETR
        models = [
            ("yolov12x.pt", YOLO),
            ("rtdetr-l.pt", RTDETR)
        ]
        for fname, loader in models:
            fpath = models_dir / fname
            if not fpath.exists():
                logger.info(f"ðŸ”„ Downloading {fname} using Ultralytics library...")
                try:
                    # FIX: Handle yolov12x fallback to yolov8x
                    if fname == "yolov12x.pt":
                        try:
                            # Try to load yolov12x first
                            model = loader("yolov12x")
                        except Exception:
                            # Fallback to yolov8x if yolov12x doesn't exist
                            logger.warning("âš ï¸ yolov12x not available, falling back to yolov8x")
                            model = loader("yolov8x")
                    else:
                        model = loader(fname.replace(".pt", ""))  # Remove extension for model name

                    # Save the model
                    if hasattr(model, 'save'):
                        model.save(str(fpath))
                    else:
                        # For older versions, use export
                        model.export(format='pt', imgsz=640)
                        exported_path = Path(model.trainer.save_dir) / "weights" / "best.pt"
                        if exported_path.exists():
                            shutil.copy(exported_path, fpath)

                    logger.info(f"âœ… Downloaded {fname}")
                except Exception as e:
                    logger.error(f"âŒ Failed to download {fname}: {e}")
            else:
                logger.info(f"âœ… {fname} already exists.")
    except ImportError as e:
        logger.error(f"âŒ Ultralytics not installed: {e}")

    # 2. MiDaS DPT_Large (torch.hub)
    try:
        import torch
        logger.info("ðŸ”„ Ensuring MiDaS (DPT_Large) weights via torch.hub...")

        # FIX: MiDaS models are downloaded to torch hub cache, not MODELS_DIR
        # Just ensure the model is cached by loading it
        torch.hub.load('intel-isl/MiDaS', 'DPT_Large', pretrained=True)
        torch.hub.load('intel-isl/MiDaS', 'transforms')

        logger.info("âœ… MiDaS (DPT_Large) weights present or downloaded.")
    except Exception as e:
        logger.error(f"âŒ Could not ensure MiDaS weights: {e}")

    # 3. Hugging Face DPT & GLPN
    try:
        from transformers import (
            DPTImageProcessor, DPTForDepthEstimation,
            GLPNImageProcessor, GLPNForDepthEstimation
        )
        logger.info("ðŸ”„ Ensuring DPT and GLPN models via Hugging Face Transformers...")
        DPTImageProcessor.from_pretrained("Intel/dpt-large")
        DPTForDepthEstimation.from_pretrained("Intel/dpt-large")
        GLPNImageProcessor.from_pretrained("vinvino02/glpn-nyu")
        GLPNForDepthEstimation.from_pretrained("vinvino02/glpn-nyu")
        logger.info("âœ… DPT and GLPN models present or downloaded.")
    except ImportError:
        logger.warning("transformers not installed, skipping Hugging Face models.")
    except Exception as e:
        logger.error(f"âŒ Could not ensure Hugging Face depth models: {e}")

    # 4. BotSort Re-ID model
    reid_path = Path.home() / ".cache/torch/checkpoints/osnet_x0_25_msmt17.pt"

    # FIX: Create directory before downloading
    if not reid_path.exists():
        logger.info("ðŸ”„ Downloading BotSort Re-ID model...")
        # FIX: Correct URL with .pth extension
        url = "https://github.com/KaiyangZhou/deep-person-reid/releases/download/v1.0/osnet_x0_25_msmt17.pth"
        try:
            reid_path.parent.mkdir(parents=True, exist_ok=True)
            urllib.request.urlretrieve(url, reid_path)
            logger.info("âœ… BotSort Re-ID model downloaded.")
        except PermissionError:
            # Fallback location if permission denied
            reid_path = Path.home() / ".cache" / "dubs_models" / "osnet_x0_25_msmt17.pt"
            reid_path.parent.mkdir(parents=True, exist_ok=True)
            urllib.request.urlretrieve(url, reid_path)
            logger.warning(f"âš ï¸ Using fallback Re-ID location: {reid_path}")
            # Update global path
            global REID_MODEL_PATH
            REID_MODEL_PATH = reid_path
        except Exception as e:
            logger.error(f"âŒ Could not download BotSort Re-ID model: {e}")
    else:
        logger.info("âœ… BotSort Re-ID model already exists.")

def ensure_models_available():
    """
    Ensure all required models are available, downloading if necessary.
    This function is called during module initialization to set up models.
    """
    # Check for critical model files
    MODEL_FILES = ["yolov12x.pt", "rtdetr-l.pt"]  # Core detection models
    missing_models = []
    for model_file in MODEL_FILES:
        if not (MODELS_DIR / model_file).exists():
            missing_models.append(model_file)

    if missing_models:
        logger.warning(f"âš ï¸ Missing models detected: {missing_models}")
        logger.info("ðŸ”„ Attempting to download missing models...")
        try:
            download_models()  # Use the new download_models function
            # Re-check after download attempt
            still_missing = []
            for model_file in MODEL_FILES:
                if not (MODELS_DIR / model_file).exists():
                    still_missing.append(model_file)
            if still_missing:
                logger.error(f"âŒ Model files still missing after download attempt: {still_missing}")
                logger.warning(f"âš ï¸ Some models are missing: {still_missing}. Processing may fail.")
            else:
                logger.info("âœ… All required models are now available")
        except Exception as e:
            logger.error(f"âŒ Model download failed: {e}")
            logger.warning(f"âš ï¸ Model download failed: {e}. Processing may fail.")
    else:
        logger.info("âœ… All required models are available")

# Initialize models on module import (but don't fail if models are missing)
try:
    ensure_models_available()
except Exception as e:
    logger.warning(f"âš ï¸ Model initialization failed: {e}. Models will be downloaded on first use.")

# ======================================
# Depth Estimator
# ======================================

class DepthEstimator:
    """Multi-tier depth estimation with fixed depth interpretation."""
    def __init__(self, device='auto'):
        if device == 'auto':
            if torch.backends.mps.is_available():
                self.device = torch.device('mps')
                logger.info("ðŸŽ Using MPS (Apple Silicon GPU) for acceleration")
            elif torch.cuda.is_available():
                self.device = torch.device('cuda')
                logger.info("ðŸš€ Using CUDA GPU for acceleration")
            else:
                self.device = torch.device('cpu')
                logger.info("ðŸ’» Using CPU")
        else:
            self.device = torch.device(device)
        self.model = None
        self.transform = None
        self.method = "fallback"
        self._load_depth_models()

    def _load_depth_models(self):
        """Try loading depth models in order of preference."""
        if self._try_load_midas():
            return
        if self._try_load_dpt_hf():
            return
        if self._try_load_glpn():
            return
        logger.warning("âš ï¸ All depth models failed. Using geometric fallback.")
        self.method = "geometric"

    def _try_load_midas(self):
        """Try loading MiDaS - primary choice for speed."""
        if not MIDAS_AVAILABLE:
            return False
        try:
            # FIX: MiDaS models are cached in torch hub directory, not MODELS_DIR
            logger.info("Loading MiDaS from torch.hub (FAST, will use cache if available)")
            self.model = torch.hub.load(MIDAS_REPO, MIDAS_MODEL_NAME, pretrained=True)
            self.model = self.model.to(self.device).eval()
            self.transform = torch.hub.load(MIDAS_REPO, 'transforms').dpt_transform
            self.method = "midas"
            logger.info("âœ… MiDaS loaded successfully (PRIMARY - optimized for speed)")
            return True
        except Exception as e:
            logger.warning(f"MiDaS loading failed: {e}")
            return False

    def _try_load_dpt_hf(self):
        """Try loading DPT via Hugging Face."""
        try:
            from transformers import DPTImageProcessor, DPTForDepthEstimation
            logger.info("Loading DPT via Hugging Face...")

            # Load directly from HF hub (models are cached by download_models function)
            logger.info("Loading DPT from Hugging Face hub...")
            self.processor = DPTImageProcessor.from_pretrained(DPT_HF_MODEL)
            self.model = DPTForDepthEstimation.from_pretrained(DPT_HF_MODEL)

            self.model = self.model.to(self.device).eval()
            self.method = "dpt_hf"
            logger.info("âœ… DPT (Hugging Face) loaded successfully")
            return True
        except ImportError:
            logger.info("transformers not available for DPT")
            return False
        except Exception as e:
            logger.warning(f"DPT (HF) loading failed: {e}")
            return False

    def _try_load_glpn(self):
        """Try loading GLPN."""
        try:
            from transformers import GLPNImageProcessor, GLPNForDepthEstimation
            logger.info("Loading GLPN (FALLBACK - slower but accurate)...")

            # Load directly from HF hub (models are cached by download_models function)
            logger.info("Loading GLPN from Hugging Face hub...")
            self.processor = GLPNImageProcessor.from_pretrained(GLPN_HF_MODEL)
            self.model = GLPNForDepthEstimation.from_pretrained(GLPN_HF_MODEL)

            self.model = self.model.to(self.device).eval()
            self.method = "glpn"
            logger.info("âœ… GLPN loaded successfully (WARNING: This will be slow ~0.3 FPS)")
            return True
        except ImportError:
            logger.info("transformers not available for GLPN")
            return False
        except Exception as e:
            logger.warning(f"GLPN loading failed: {e}")
            return False

    def estimate_depth(self, frame: np.ndarray) -> np.ndarray:
        """Estimate depth using the best available method (0=far, 1=close)."""
        if self.method == "geometric":
            return self._geometric_depth(frame)
        if self.model is None:
            return self._geometric_depth(frame)
        try:
            if self.method == "midas":
                return self._estimate_midas(frame)
            elif self.method == "dpt_hf":
                return self._estimate_dpt_hf(frame)
            elif self.method == "glpn":
                return self._estimate_glpn(frame)
            else:
                return self._geometric_depth(frame)
        except Exception as e:
            logger.warning(f"{self.method} depth estimation failed: {e}. Using geometric fallback.")
            return self._geometric_depth(frame)

    def _estimate_midas(self, frame: np.ndarray) -> np.ndarray:
        """MiDaS depth estimation."""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        input_tensor = self.transform(rgb).to(self.device)
        with torch.no_grad():
            prediction = self.model(input_tensor)
            if prediction.dim() == 3:
                prediction = prediction.unsqueeze(0)
            depth_map = F.interpolate(
                prediction,
                size=frame.shape[:2],
                mode='bicubic',
                align_corners=False
            ).squeeze().cpu().numpy()
        if depth_map.max() > depth_map.min():
            depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
        else:
            depth_map = np.full_like(depth_map, 0.5)
        return depth_map.astype(np.float32)

    def _estimate_dpt_hf(self, frame: np.ndarray) -> np.ndarray:
        """DPT depth estimation via Hugging Face."""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        inputs = self.processor(images=rgb, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
            predicted_depth = outputs.predicted_depth
        depth_map = F.interpolate(
            predicted_depth.unsqueeze(1),
            size=frame.shape[:2],
            mode='bicubic',
            align_corners=False
        ).squeeze().cpu().numpy()
        if depth_map.max() > depth_map.min():
            depth_map = (depth_map.max() - depth_map) / (depth_map.max() - depth_map.min())
        else:
            depth_map = np.full_like(depth_map, 0.5)
        return depth_map.astype(np.float32)

    def _estimate_glpn(self, frame: np.ndarray) -> np.ndarray:
        """GLPN depth estimation."""
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        inputs = self.processor(images=rgb, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
            predicted_depth = outputs.predicted_depth
        depth_map = F.interpolate(
            predicted_depth.unsqueeze(1),
            size=frame.shape[:2],
            mode='bicubic',
            align_corners=False
        ).squeeze().cpu().numpy()
        if depth_map.max() > depth_map.min():
            depth_map = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min())
        else:
            depth_map = np.full_like(depth_map, 0.5)
        return depth_map.astype(np.float32)

    def _geometric_depth(self, frame: np.ndarray) -> np.ndarray:
        """Geometric fallback: bottom=close, top=far."""
        h, w = frame.shape[:2]
        depth_map = np.zeros((h, w), dtype=np.float32)
        for y in range(h):
            for x in range(w):
                vertical_depth = (h - y) / h
                vertical_depth = 1.0 - vertical_depth
                center_x = w / 2
                horizontal_factor = 1.0 - abs(x - center_x) / center_x * 0.3
                depth_value = vertical_depth * horizontal_factor
                depth_map[y, x] = np.clip(depth_value, 0.0, 1.0)
        return depth_map

# ======================================
# Smart Adaptive Detector
# ======================================

class SmartAdaptiveDetector:
    """Smart detector with depth-aware strategies."""
    def __init__(self, device='auto'):
        if device == 'auto':
            if torch.backends.mps.is_available():
                self.device = 'mps'
            elif torch.cuda.is_available():
                # Fix: Use explicit device index for CUDA
                self.device = 'cuda:0'
            else:
                self.device = 'cpu'
        else:
            self.device = device
        self.yolo_conf_close = YOLO_CONF_CLOSE
        self.yolo_conf_medium = YOLO_CONF_MEDIUM
        self.yolo_conf_far = YOLO_CONF_FAR
        self.rtdetr_conf_close = RTDETR_CONF_CLOSE
        self.rtdetr_conf_medium = RTDETR_CONF_MEDIUM
        self.rtdetr_conf_far = RTDETR_CONF_FAR
        try:
            self.yolo = load_yolo_model(YOLO_MODEL_PATH)
            logger.info("âœ… YOLO loaded")
        except Exception as e:
            self.yolo = None
            logger.error(f"âŒ Failed to load YOLO: {e}")
        try:
            self.rtdetr = RTDETR(RTDETR_MODEL_PATH)
            logger.info("âœ… RT-DETR loaded")
        except Exception as e:
            self.rtdetr = None
            logger.error(f"âŒ Failed to load RT-DETR: {e}")
        self._last_strategy = None

    def choose_detection_strategy(self, depth_map):
        """Choose optimal detection strategy based on depth characteristics."""
        depth_std = np.std(depth_map)
        depth_mean = np.mean(depth_map)
        depth_range = np.max(depth_map) - np.min(depth_map)
        variance_coeff = depth_std / depth_mean if depth_mean > 0 else 0
        logger.debug(f"Depth analysis: mean={depth_mean:.3f}, std={depth_std:.3f}, range={depth_range:.3f}, var_coeff={variance_coeff:.3f}")
        if variance_coeff < 0.3 and depth_range < 0.4:
            if depth_mean > 0.7:
                self._last_strategy = "yolo_only"
                reason = f"All objects close (mean={depth_mean:.2f})"
            elif depth_mean < 0.3:
                self._last_strategy = "rtdetr_only"
                reason = f"All objects far (mean={depth_mean:.2f})"
            else:
                self._last_strategy = "hybrid_simple"
                reason = f"All objects medium distance (mean={depth_mean:.2f})"
        else:
            self._last_strategy = "zone_based"
            reason = f"Mixed depths detected (std={depth_std:.2f}, range={depth_range:.2f})"
        logger.info(f"ðŸŽ¯ Detection strategy: {self._last_strategy.upper()} - {reason}")
        return self._last_strategy

    def _extract_detections(self, results, conf_thresh):
        """Extract person detections from model results."""
        if not hasattr(results, 'boxes') or results.boxes is None or len(results.boxes) == 0:
            return np.empty((0, 6))
        boxes = results.boxes
        xyxy, conf, cls = boxes.xyxy.cpu().numpy(), boxes.conf.cpu().numpy(), boxes.cls.cpu().numpy()
        person_mask = (cls == 0) & (conf >= conf_thresh)
        detections = np.hstack((xyxy[person_mask], conf[person_mask][:, None], cls[person_mask][:, None]))
        if len(detections) == 0:
            return np.empty((0, 6))
        areas = (detections[:, 2] - detections[:, 0]) * (detections[:, 3] - detections[:, 1])
        return detections[areas > 200]

    def detect_yolo_only(self, frame):
        """YOLO-only detection for close objects."""
        if self.yolo is None:
            return np.empty((0, 6))
        try:
            results = self.yolo(frame, conf=self.yolo_conf_close, classes=[0], verbose=False)[0]
            detections = self._extract_detections(results, self.yolo_conf_close)
            logger.debug(f"YOLO-only: {len(detections)} detections")
            return detections
        except Exception as e:
            logger.warning(f"YOLO-only detection failed: {e}")
            return np.empty((0, 6))

    def detect_rtdetr_only(self, frame):
        """RT-DETR-only detection for far objects."""
        if self.rtdetr is None:
            return np.empty((0, 6))
        try:
            results = self.rtdetr(frame, conf=self.rtdetr_conf_far, classes=[0], verbose=False)[0]
            detections = self._extract_detections(results, self.rtdetr_conf_far)
            logger.debug(f"RT-DETR-only: {len(detections)} detections")
            return detections
        except Exception as e:
            logger.warning(f"RT-DETR-only detection failed: {e}")
            return np.empty((0, 6))

    def detect_hybrid_simple(self, frame):
        """Simple hybrid detection for medium-distance scenarios."""
        all_detections = []
        if self.yolo is not None:
            try:
                yolo_results = self.yolo(frame, conf=self.yolo_conf_medium, classes=[0], verbose=False)[0]
                yolo_dets = self._extract_detections(yolo_results, self.yolo_conf_medium)
                if len(yolo_dets) > 0:
                    all_detections.append(yolo_dets)
            except Exception as e:
                logger.warning(f"YOLO detection failed in hybrid: {e}")
        if self.rtdetr is not None:
            try:
                rtdetr_results = self.rtdetr(frame, conf=self.rtdetr_conf_medium, classes=[0], verbose=False)[0]
                rtdetr_dets = self._extract_detections(rtdetr_results, self.rtdetr_conf_medium)
                if len(rtdetr_dets) > 0:
                    all_detections.append(rtdetr_dets)
            except Exception as e:
                logger.warning(f"RT-DETR detection failed in hybrid: {e}")
        if all_detections:
            combined = np.vstack(all_detections)
            final_dets = self._apply_nms(combined)
            logger.debug(f"Hybrid-simple: {len(final_dets)} detections after NMS")
            return final_dets
        return np.empty((0, 6))

    def detect_zone_based(self, frame, depth_map):
        """Zone-based detection for mixed-depth scenarios."""
        if depth_map is None:
            return self.detect_hybrid_simple(frame)
        threshold_far = np.percentile(depth_map, 33)
        threshold_close = np.percentile(depth_map, 67)
        logger.debug(f"Adaptive thresholds: far<={threshold_far:.2f}, close>={threshold_close:.2f}")
        height, width = frame.shape[:2]
        all_detections = []
        if self.yolo is not None:
            try:
                yolo_results = self.yolo(frame, conf=self.yolo_conf_close, classes=[0], verbose=False)[0]
                yolo_dets = self._extract_detections(yolo_results, self.yolo_conf_close)
                if len(yolo_dets) > 0:
                    filtered_yolo = []
                    for det in yolo_dets:
                        x1, y1, x2, y2 = det[:4]
                        center_x, center_y = int((x1 + x2) / 2), int((y1 + y2) / 2)
                        if 0 <= center_y < height and 0 <= center_x < width:
                            depth_value = depth_map[center_y, center_x]
                            if depth_value >= threshold_close:
                                filtered_yolo.append(det)
                    if filtered_yolo:
                        all_detections.append(np.array(filtered_yolo))
                        logger.debug(f"YOLO (CLOSE): {len(filtered_yolo)} detections")
            except Exception as e:
                logger.warning(f"YOLO detection failed in zones: {e}")
        if self.rtdetr is not None:
            try:
                rtdetr_results = self.rtdetr(frame, conf=self.rtdetr_conf_far, classes=[0], verbose=False)[0]
                rtdetr_dets = self._extract_detections(rtdetr_results, self.rtdetr_conf_far)
                if len(rtdetr_dets) > 0:
                    filtered_rtdetr = []
                    for det in rtdetr_dets:
                        x1, y1, x2, y2 = det[:4]
                        center_x, center_y = int((x1 + x2) / 2), int((y1 + y2) / 2)
                        if 0 <= center_y < height and 0 <= center_x < width:
                            depth_value = depth_map[center_y, center_x]
                            if depth_value <= threshold_far:
                                filtered_rtdetr.append(det)
                    if filtered_rtdetr:
                        all_detections.append(np.array(filtered_rtdetr))
                        logger.debug(f"RT-DETR (FAR): {len(filtered_rtdetr)} detections")
            except Exception as e:
                logger.warning(f"RT-DETR detection failed in zones: {e}")
        if self.yolo is not None and self.rtdetr is not None:
            try:
                yolo_all = self._extract_detections(
                    self.yolo(frame, conf=self.yolo_conf_medium, classes=[0], verbose=False)[0],
                    self.yolo_conf_medium
                )
                rtdetr_all = self._extract_detections(
                    self.rtdetr(frame, conf=self.rtdetr_conf_medium, classes=[0], verbose=False)[0],
                    self.rtdetr_conf_medium
                )
                middle_detections = []
                for det_list in [yolo_all, rtdetr_all]:
                    if len(det_list) > 0:
                        for det in det_list:
                            x1, y1, x2, y2 = det[:4]
                            center_x, center_y = int((x1 + x2) / 2), int((y1 + y2) / 2)
                            if 0 <= center_y < height and 0 <= center_x < width:
                                depth_value = depth_map[center_y, center_x]
                                if threshold_far < depth_value < threshold_close:
                                    middle_detections.append(det)
                if middle_detections:
                    middle_array = np.array(middle_detections)
                    middle_nms = self._apply_nms(middle_array, iou_thresh=0.3)
                    all_detections.append(middle_nms)
                    logger.debug(f"ENSEMBLE (MIDDLE): {len(middle_nms)} detections")
            except Exception as e:
                logger.warning(f"Ensemble detection failed: {e}")
        if all_detections:
            combined = np.vstack(all_detections)
            final_dets = self._apply_nms(combined)
            logger.debug(f"Zone-based total: {len(final_dets)} detections after final NMS")
            return final_dets
        return np.empty((0, 6))

    def detect_smart_adaptive(self, frame, depth_map=None):
        """Main detection method with optimal strategy."""
        if depth_map is None:
            return self.detect_hybrid_simple(frame)
        strategy = self.choose_detection_strategy(depth_map)
        if strategy == "yolo_only":
            return self.detect_yolo_only(frame)
        elif strategy == "rtdetr_only":
            return self.detect_rtdetr_only(frame)
        elif strategy == "hybrid_simple":
            return self.detect_hybrid_simple(frame)
        elif strategy == "zone_based":
            return self.detect_zone_based(frame, depth_map)
        return self.detect_hybrid_simple(frame)

    def _apply_nms(self, detections, iou_thresh=DEFAULT_NMS_IOU_THRESH):
        """Apply Non-Maximum Suppression."""
        if len(detections) == 0:
            return detections
        x1, y1, x2, y2, scores = detections[:, 0], detections[:, 1], detections[:, 2], detections[:, 3], detections[:, 4]
        areas = (x2 - x1) * (y2 - y1)
        order = scores.argsort()[::-1]
        keep = []
        while order.size > 0:
            i = order[0]
            keep.append(i)
            if len(order) == 1:
                break
            xx1, yy1 = np.maximum(x1[i], x1[order[1:]]), np.maximum(y1[i], y1[order[1:]])
            xx2, yy2 = np.minimum(x2[i], x2[order[1:]]), np.minimum(y2[i], y2[order[1:]])
            w, h = np.maximum(0.0, xx2 - xx1), np.maximum(0.0, yy2 - yy1)
            inter = w * h
            ovr = inter / (areas[i] + areas[order[1:]] - inter + 1e-5)
            inds = np.where(ovr <= iou_thresh)[0]
            order = order[inds + 1]
        return detections[keep]

# ======================================
# Post-Processing Engine
# ======================================

class PostProcessingEngine:
    """Enhanced post-processing for track merging."""
    def __init__(self, config=None):
        self.config = config or self._default_config()

    def _default_config(self):
        """Default configuration parameters."""
        return {
            'max_time_gap_seconds': 15.0,
            'max_distance_pixels': 350.0,
            'min_size_ratio': 0.4,
            'reid_threshold_high': 0.70,
            'reid_threshold_medium': 0.50,
            'reid_threshold_low': 0.30,
            'enable_motion_prediction': True,
            'enable_multi_frame_averaging': True,
            'verbose_logging': False
        }

    def _cosine_similarity(self, a, b):
        """Compute cosine similarity with scipy fallback."""
        if SCIPY_AVAILABLE:
            try:
                return 1.0 - cosine(a, b)
            except Exception as e:
                logger.warning(f"Failed to compute cosine similarity: {e}")
                # Fall back to manual calculation
        dot_product = np.dot(a, b)
        norm_a = np.linalg.norm(a)
        norm_b = np.linalg.norm(b)
        if norm_a == 0 or norm_b == 0:
            return 0.0
        return dot_product / (norm_a * norm_b)

    def process_tracks(self, track_history, fps, tracker=None):
        """Main entry point for track merging with lifetime filtering."""
        logger.info("ðŸš€ Enhanced Post-Processing: Applying Lifetime Filter + Merging...")
        if not track_history:
            return {}, 0
        filtered_history = {}
        removed_ids = []
        for track_id, track_data in track_history.items():
            if len(track_data) >= MIN_LIFETIME_FRAMES:
                filtered_history[track_id] = track_data
            else:
                removed_ids.append(track_id)
        if removed_ids:
            logger.warning(f"LIFETIME FILTER: Removed {len(removed_ids)} short-lived ghost tracks: {removed_ids}")
        track_history = filtered_history
        for track_id in track_history:
            track_history[track_id].sort(key=lambda x: x['frame'])
        track_features = self._extract_and_average_features(track_history, tracker)
        track_motion = self._analyze_track_motion(track_history, fps)
        merge_map = self._hierarchical_track_merging(track_history, track_features, track_motion, fps)
        final_id_map, final_count = self._apply_merges_and_generate_ids(track_history, merge_map)
        logger.info(f"âœ… Enhanced post-processing complete: {len(track_history)} â†’ {final_count} tracks after merging.")
        return final_id_map, final_count

    def _extract_and_average_features(self, track_history, tracker):
        """Extract and average Re-ID features."""
        track_features = {}
        for track_id, track_data in track_history.items():
            features_list = []
            for frame_data in track_data:
                if 'reid_features' in frame_data and frame_data['reid_features'] is not None:
                    features_list.append(frame_data['reid_features'])
            if features_list and len(features_list) >= 2:
                if len(features_list) >= 5:
                    start_idx = len(features_list) // 4
                    end_idx = 3 * len(features_list) // 4
                    selected_features = features_list[start_idx:end_idx]
                else:
                    selected_features = features_list
                avg_features = np.mean(selected_features, axis=0)
                if np.linalg.norm(avg_features) > 0:
                    avg_features = avg_features / np.linalg.norm(avg_features)
                track_features[track_id] = {
                    'features': avg_features,
                    'confidence': min(1.0, len(selected_features) / 5.0),
                    'frame_count': len(selected_features)
                }
            else:
                track_features[track_id] = {
                    'features': None,
                    'confidence': 0.0,
                    'frame_count': 0
                }
        return track_features

    def _analyze_track_motion(self, track_history, fps):
        """Analyze motion patterns and predict positions."""
        track_motion = {}
        for track_id, track_data in track_history.items():
            if len(track_data) < 2:
                track_motion[track_id] = {
                    'velocity': np.array([0.0, 0.0]),
                    'acceleration': np.array([0.0, 0.0]),
                    'stability': 0.0,
                    'predicted_next_pos': None
                }
                continue
            positions = []
            times = []
            for frame_data in track_data:
                bbox = frame_data['bbox']
                center_x = (bbox[0] + bbox[2]) / 2
                center_y = (bbox[1] + bbox[3]) / 2
                positions.append([center_x, center_y])
                times.append(frame_data['frame'] / fps)
            positions = np.array(positions)
            times = np.array(times)
            if len(positions) >= 3:
                recent_positions = positions[-3:]
                recent_times = times[-3:]
                velocity = self._calculate_velocity(recent_positions, recent_times)
            else:
                velocity = (positions[-1] - positions[0]) / (times[-1] - times[0]) if times[-1] != times[0] else np.array([0.0, 0.0])
            if len(positions) >= 4:
                velocities = []
                for i in range(1, len(positions)):
                    v = (positions[i] - positions[i - 1]) / (times[i] - times[i - 1]) if times[i] != times[i - 1] else np.array([0.0, 0.0])
                    velocities.append(v)
                velocities = np.array(velocities)
                stability = 1.0 / (1.0 + np.std(velocities)) if len(velocities) > 0 else 0.5
            else:
                stability = 0.5
            last_time = times[-1]
            predicted_pos = positions[-1] + velocity * self.config['max_time_gap_seconds']
            track_motion[track_id] = {
                'velocity': velocity,
                'stability': stability,
                'predicted_next_pos': predicted_pos,
                'last_position': positions[-1],
                'last_time': last_time
            }
        return track_motion

    def _calculate_velocity(self, positions, times):
        """Calculate robust velocity."""
        if len(positions) < 2:
            return np.array([0.0, 0.0])
        dt = times[-1] - times[0]
        if dt == 0:
            return np.array([0.0, 0.0])
        dx = positions[-1][0] - positions[0][0]
        dy = positions[-1][1] - positions[0][1]
        return np.array([dx / dt, dy / dt])

    def _hierarchical_track_merging(self, track_history, track_features, track_motion, fps):
        """Hierarchical similarity-based track merging."""
        merge_map = {}
        sorted_ids = sorted(track_history.keys())
        time_threshold_frames = int(self.config['max_time_gap_seconds'] * fps)
        for i in range(len(sorted_ids)):
            for j in range(i + 1, len(sorted_ids)):
                id1, id2 = sorted_ids[i], sorted_ids[j]
                if id1 in merge_map or id2 in merge_map:
                    continue
                track1, track2 = track_history[id1], track_history[id2]
                if not track1 or not track2:
                    continue
                end_of_track1 = track1[-1]
                start_of_track2 = track2[0]
                frame_gap = start_of_track2['frame'] - end_of_track1['frame']
                if not (0 < frame_gap <= time_threshold_frames):
                    continue
                similarity_score = self._compute_hierarchical_similarity(id1, id2, track1, track2, track_features, track_motion, frame_gap)
                should_merge = False
                merge_reason = ""
                if similarity_score['reid_similarity'] >= self.config['reid_threshold_high']:
                    should_merge = True
                    merge_reason = f"High Re-ID similarity ({similarity_score['reid_similarity']:.2f})"
                elif similarity_score['reid_similarity'] >= self.config['reid_threshold_medium']:
                    if similarity_score['spatial_score'] > 0.6:
                        should_merge = True
                        merge_reason = f"Medium Re-ID + Good spatial ({similarity_score['reid_similarity']:.2f}, {similarity_score['spatial_score']:.2f})"
                elif similarity_score['reid_similarity'] >= self.config['reid_threshold_low']:
                    if similarity_score['motion_score'] > 0.7 and similarity_score['spatial_score'] > 0.5:
                        should_merge = True
                        merge_reason = f"Low Re-ID + Strong motion + Spatial ({similarity_score['reid_similarity']:.2f}, {similarity_score['motion_score']:.2f})"
                else:
                    if similarity_score['reid_similarity'] == 0.0:
                        if similarity_score['spatial_score'] > 0.8 and similarity_score['motion_score'] > 0.6:
                            should_merge = True
                            merge_reason = f"Geometric fallback ({similarity_score['spatial_score']:.2f}, {similarity_score['motion_score']:.2f})"
                if should_merge:
                    merge_map[id2] = id1
                    if self.config['verbose_logging']:
                        logger.info(f"Merging track {id2} â†’ {id1}: {merge_reason}")
        return merge_map

    def _compute_hierarchical_similarity(self, id1, id2, track1, track2, track_features, track_motion, frame_gap):
        """Compute multi-dimensional similarity between tracks."""
        reid_sim = 0.0
        if (track_features[id1]['features'] is not None and track_features[id2]['features'] is not None):
            try:
                reid_sim = self._cosine_similarity(track_features[id1]['features'], track_features[id2]['features'])
                reid_sim = max(0.0, reid_sim)
                confidence_weight = (track_features[id1]['confidence'] + track_features[id2]['confidence']) / 2
                reid_sim *= confidence_weight
            except Exception:
                reid_sim = 0.0
        end_bbox1 = track1[-1]['bbox']
        start_bbox2 = track2[0]['bbox']
        center1 = np.array([(end_bbox1[0] + end_bbox1[2]) / 2, (end_bbox1[1] + end_bbox1[3]) / 2])
        center2 = np.array([(start_bbox2[0] + start_bbox2[2]) / 2, (start_bbox2[1] + start_bbox2[3]) / 2])
        distance = np.linalg.norm(center1 - center2)
        spatial_score = max(0.0, 1.0 - distance / self.config['max_distance_pixels'])
        area1 = (end_bbox1[2] - end_bbox1[0]) * (end_bbox1[3] - end_bbox1[1])
        area2 = (start_bbox2[2] - start_bbox2[0]) * (start_bbox2[3] - start_bbox2[1])
        size_ratio = min(area1, area2) / max(area1, area2) if max(area1, area2) > 0 else 0
        spatial_score = 0.7 * spatial_score + 0.3 * size_ratio
        motion_score = 0.0
        if (self.config['enable_motion_prediction'] and track_motion[id1]['predicted_next_pos'] is not None):
            predicted_pos = track_motion[id1]['predicted_next_pos']
            actual_pos = center2
            prediction_error = np.linalg.norm(predicted_pos - actual_pos)
            motion_score = max(0.0, 1.0 - prediction_error / self.config['max_distance_pixels'])
            stability_factor = (track_motion[id1]['stability'] + track_motion[id2]['stability']) / 2
            motion_score *= stability_factor
        return {
            'reid_similarity': reid_sim,
            'spatial_score': spatial_score,
            'motion_score': motion_score,
            'overall_score': 0.5 * reid_sim + 0.3 * spatial_score + 0.2 * motion_score
        }

    def _apply_merges_and_generate_ids(self, track_history, merge_map):
        """Apply merges and generate sequential IDs."""
        sorted_ids = sorted(track_history.keys())
        final_id_map = {}
        sequential_id_counter = 1
        root_to_final_id = {}
        for original_id in sorted_ids:
            current_id = original_id
            path = [original_id]
            while current_id in merge_map:
                current_id = merge_map[current_id]
                path.append(current_id)
            root_id = current_id
            if root_id not in root_to_final_id:
                root_to_final_id[root_id] = sequential_id_counter
                sequential_id_counter += 1
            final_id = root_to_final_id[root_id]
            for node_id in path:
                final_id_map[node_id] = final_id
        final_person_count = sequential_id_counter - 1
        return final_id_map, final_person_count

# ======================================
# Main Processing Class
# ======================================

class DubsComprehensivePeopleCounting:
    """Comprehensive people counting with adaptive detection and post-processing."""
    def __init__(self, device='auto', use_reid=DEFAULT_USE_REID):
        # Ensure models are available before initialization
        try:
            ensure_models_available()
        except Exception as e:
            logger.warning(f"âš ï¸ Model availability check failed: {e}")

        if device == 'auto':
            if torch.backends.mps.is_available():
                self.device = 'mps'
                logger.info("ðŸŽ Using MPS (Apple Silicon GPU) for acceleration")
            elif torch.cuda.is_available():
                # Fix: Use explicit device index for CUDA
                self.device = 'cuda:0'
                logger.info("ðŸš€ Using CUDA GPU (device 0) for acceleration")
            else:
                self.device = 'cpu'
                logger.info("ðŸ’» Using CPU")
        else:
            self.device = str(device)
        logger.info(f"Using device: {self.device}")

        self.depth_estimator = DepthEstimator(device)
        self.detector = SmartAdaptiveDetector(self.device)

        try:
            if use_reid:
                logger.info("âœ… Initializing BoTSORT with Re-ID enabled...")
                self.tracker = BotSort(
                    reid_weights=REID_MODEL_PATH,
                    device=self.device,
                    half=False,
                    track_buffer=300,
                    appearance_thresh=0.60
                )
            else:
                logger.info("âœ… Initializing ByteTrack (Re-ID disabled)...")
                # Import ByteTrack here to avoid import issues
                try:
                    from boxmot import ByteTrack
                    self.tracker = ByteTrack(
                        track_buffer=150,
                        match_thresh=0.65,
                        frame_rate=25
                    )
                except ImportError:
                    logger.warning("âš ï¸ ByteTrack not available, using BotSortLite (BotSort without Re-ID)")
                    self.tracker = BotSort(
                        reid_weights=None,
                        device=self.device,
                        half=False,
                        track_buffer=150,
                        appearance_thresh=0.60
                    )
        except Exception as e:
            self.tracker = None
            logger.error(f"âŒ Failed to initialize tracker: {e}")

    def process_image(self, image_path: str) -> Dict:
        """Process a single image for people counting."""
        start_time = time.time()
        # Validate with full path, but get filename for storage operations
        is_valid, error_msg = validate_input_file(image_path)
        if not is_valid:
            logger.error(f"Invalid input: {error_msg}")
            return {
                'status': 'failed',
                'job_type': 'people_count',
                'output_image': None,
                'output_video': None,
                'data': {'alerts': [], 'error': error_msg},
                'meta': {'timestamp': timezone.now().isoformat(), 'processing_time_seconds': time.time() - start_time},
                'error': {'message': error_msg, 'code': 'INVALID_INPUT'}
            }

        # Get just the filename for storage operati        # Remove the line that extracts only filename
        # image_filename = Path(image_path).name

        try:
            with default_storage.open(image_path, 'rb') as f:
                frame = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)
            depth_map = self.depth_estimator.estimate_depth(frame)
            detections = self.detector.detect_smart_adaptive(frame, depth_map)
            person_count = len(detections)
            alerts = []
            if person_count > 10:
                alerts.append({
                    "message": f"High crowd density detected: {person_count} people",
                    "timestamp": timezone.now().isoformat()
                })
            if self.tracker:
                self.tracker.reset()
                tracks = self.tracker.update(detections, frame) if len(detections) > 0 else np.empty((0, 7))
                person_count = len(tracks) if len(tracks) > 0 else person_count
            # Create output file in temporary location for tasks.py to handle
            with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as tmp:
                if len(detections) > 0:
                    results = self.detector.yolo(frame, conf=YOLO_CONF_CLOSE, classes=[0], verbose=False)[0] if self.detector.yolo else \
                              self.detector.rtdetr(frame, conf=RTDETR_CONF_CLOSE, classes=[0], verbose=False)[0]
                    annotated_frame = results.plot()
                else:
                    annotated_frame = frame
                cv2.imwrite(tmp.name, annotated_frame)
                output_file_path = tmp.name
            processing_time = time.time() - start_time
            return {
                'status': 'completed',
                'job_type': 'people_count',
                'output_image': output_file_path,
                'output_video': None,
                'data': {
                    'person_count': person_count,
                    'raw_track_count': len(detections),
                    'depth_method': self.depth_estimator.method,
                    'fps': None,
                    'detection_strategies': [self.detector._last_strategy] if self.detector._last_strategy else ['unknown'],
                    'alerts': alerts
                },
                'meta': {
                    'timestamp': timezone.now().isoformat(),
                    'processing_time_seconds': processing_time,
                    'fps': None,
                    'frame_count': 1
                },
                'error': None
            }
        except Exception as e:
            logger.exception(f"Image processing failed: {str(e)}")
            return {
                'status': 'failed',
                'job_type': 'people_count',
                'output_image': None,
                'output_video': None,
                'data': {'alerts': [], 'error': str(e)},
                'meta': {'timestamp': timezone.now().isoformat(), 'processing_time_seconds': time.time() - start_time},
                'error': {'message': str(e), 'code': 'PROCESSING_ERROR'}
            }
        finally:
            # Note: tmp files are not cleaned up here as tasks.py needs them
            # tasks.py will handle cleanup after saving to Django storage
            pass

    def process_video(self, video_path: str, output_path: str, job_id: str = None) -> Dict:
        """Process video or image sequence for people counting."""
        start_time = time.time()
        # Validate with full path, but get filename for storage operations
        is_valid, error_msg = validate_input_file(video_path)
        if not is_valid:
            logger.error(f"Invalid input: {error_msg}")
            return {
                'status': 'failed',
                'job_type': 'people_count',
                'output_image': None,
                'output_video': None,
                'data': {'alerts': [], 'error': error_msg},
                'meta': {'timestamp': timezone.now().isoformat(), 'processing_time_seconds': time.time() - start_time},
                'error': {'message': error_msg, 'code': 'INVALID_INPUT'}
            }

        # Get just the filename for storage operations
        # Remove the line that extracts only filename
        # video_filename = Path(video_path).name

        try:
            image_files = []
            if default_storage.exists(video_path) and os.path.splitext(video_path)[1] == "":
                image_files = sorted([f for f in default_storage.listdir(video_path)[1] if f.endswith(('.jpg', '.jpeg', '.png'))])
                if not image_files:
                    raise ValueError(f"No JPG images found in directory: {video_path}")
                with default_storage.open(os.path.join(video_path, image_files[0]), 'rb') as f:
                    frame = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)
                height, width = frame.shape[:2]
                fps = 25
                total_frames = len(image_files)
                logger.info(f"ðŸ“‚ Processing image sequence: {total_frames} images")
                cap = None
            else:
                with default_storage.open(video_path, 'rb') as f:
                    with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:
                        tmp.write(f.read())
                        tmp_path = tmp.name
                cap = cv2.VideoCapture(tmp_path)
                if not cap.isOpened():
                    raise ValueError(f"Cannot open video: {video_path}")
                width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
                height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
                fps = cap.get(cv2.CAP_PROP_FPS) or 25
                total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                logger.info(f"ðŸŽ¥ Processing video: {total_frames} frames at {fps} FPS")

            # Extract job ID from the tracking_video function parameter first, then fallback to filename
            if job_id:
                output_job_id = str(job_id)
                logger.info(f"ðŸ” Using passed job_id parameter: {output_job_id}")
            else:
                # Fallback to filename extraction
                extracted_id = re.search(r'(\d+)', video_path)
                output_job_id = extracted_id.group(1) if extracted_id else str(int(time.time()))
                logger.info(f"ðŸ” Using extracted job_id from filename: {output_job_id}")

            output_filename = f"outputs/dubs_comprehensive_output_{output_job_id}.mp4"
            logger.info(f"ðŸ“ Output will be saved to: {output_filename}")
            with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp_out:
                writer = cv2.VideoWriter(tmp_out.name, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))
                all_tracks_by_frame = {}
                depth_method_used = None
                detection_strategies_used = set()
                alerts = []
                if self.tracker:
                    self.tracker.reset()
                frame_count = 0
                last_log_time = start_time

                for frame_num in tqdm(range(1, total_frames + 1), desc="Pass 1: Smart Detection"):
                    frame_count += 1
                    if cap:
                        ret, frame = cap.read()
                        if not ret:
                            break
                    else:
                        with default_storage.open(os.path.join(video_path, image_files[frame_num - 1]), 'rb') as f:
                            frame = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)
                    depth_map = self.depth_estimator.estimate_depth(frame)
                    if depth_method_used is None:
                        depth_method_used = self.depth_estimator.method
                        logger.info(f"ðŸŽ¯ Using depth estimation method: {depth_method_used}")
                    detections = self.detector.detect_smart_adaptive(frame, depth_map)
                    if self.detector._last_strategy:
                        detection_strategies_used.add(self.detector._last_strategy)
                    if len(detections) > 0 and self.tracker:
                        tracks = self.tracker.update(detections, frame)
                        track_data = []
                        if len(tracks) > 0:
                            for t in tracks:
                                track_info = {'id': int(t[4]), 'bbox': t[:4]}
                                if len(t) > 7 and t[-1] is not None:
                                    track_info['reid_features'] = t[-1]
                                x1, y1, x2, y2 = t[:4]
                                center_x, center_y = int((x1 + x2) / 2), int((y1 + y2) / 2)
                                if depth_map is not None and 0 <= center_y < height and 0 <= center_x < width:
                                    depth_value = depth_map[center_y, center_x]
                                    track_info['depth'] = depth_value
                                track_data.append(track_info)
                            all_tracks_by_frame[frame_num] = track_data
                        if len(track_data) > 10:
                            alerts.append({
                                "message": f"High crowd density detected: {len(track_data)} people at frame {frame_num}",
                                "timestamp": timezone.now().isoformat()
                            })
                    # Periodic logging with frame-level summaries
                    current_time = time.time()
                    if current_time - last_log_time >= 1 or frame_num == total_frames:  # Update every second
                        progress = (frame_num / total_frames) * 100
                        elapsed_time = current_time - start_time
                        time_remaining = (elapsed_time / frame_num) * (total_frames - frame_num) if frame_num > 0 else 0
                        avg_fps = frame_num / elapsed_time if elapsed_time > 0 else 0

                        # Frame-level detection summary
                        current_detections = len(detections)
                        current_tracks = len(track_data) if 'track_data' in locals() else 0
                        strategy_used = self.detector._last_strategy or "unknown"

                        # FIX: Update progress in the progress monitor, not the tracker
                        if hasattr(self, 'progress_monitor'):
                            self.progress_monitor.update(frame_num, f"Processing frame {frame_num}/{total_frames}")

                        # Log detailed status
                        logger.info(f"**Job {output_job_id}**: Progress **{progress:.1f}%** ({frame_num}/{total_frames}), Status: Processing...")
                        logger.info(f"[{'#' * int(progress // 10)}{'-' * (10 - int(progress // 10))}] Done: {int(elapsed_time // 60):02d}:{int(elapsed_time % 60):02d} | Left: {int(time_remaining // 60):02d}:{int(time_remaining % 60):02d} | Avg FPS: {avg_fps:.1f}")
                        logger.info(f"ðŸ“Š Frame {frame_num}: {current_detections} detections â†’ {current_tracks} tracks | Strategy: {strategy_used.upper()}")

                        last_log_time = current_time
                if cap:
                    cap.release()
                track_history_for_merge = {}
                for frame_num, frame_data in all_tracks_by_frame.items():
                    for track_info in frame_data:
                        tid = track_info['id']
                        if tid not in track_history_for_merge:
                            track_history_for_merge[tid] = []
                        track_history_for_merge[tid].append({
                            'frame': frame_num,
                            'bbox': track_info['bbox'],
                            'depth': track_info.get('depth', 0.5),
                            'reid_features': track_info.get('reid_features')
                        })
                if track_history_for_merge:
                    processing_engine = PostProcessingEngine()
                    final_id_map, final_count = processing_engine.process_tracks(track_history_for_merge, fps, self.tracker)
                    logger.info(f"âœ… Enhanced post-processing enabled: {len(track_history_for_merge)} â†’ {final_count} tracks")
                else:
                    final_count = 0
                    final_id_map = {}
                if cap is None and image_files:
                    frame = cv2.imread(image_files[0])
                    height, width = frame.shape[:2]
                else:
                    cap = cv2.VideoCapture(tmp_path)
                for frame_num in tqdm(range(1, total_frames + 1), desc="Pass 2: Rendering"):
                    if cap:
                        ret, frame = cap.read()
                        if not ret:
                            break
                    else:
                        with default_storage.open(os.path.join(video_path, image_files[frame_num - 1]), 'rb') as f:
                            frame = cv2.imdecode(np.frombuffer(f.read(), np.uint8), cv2.IMREAD_COLOR)
                    if frame_num in all_tracks_by_frame:
                        for track_info in all_tracks_by_frame[frame_num]:
                            original_id = track_info['id']
                            clean_id = final_id_map.get(original_id, original_id)
                            if clean_id:
                                x1, y1, x2, y2 = track_info['bbox']
                                depth_value = track_info.get('depth', 0.5)
                                if depth_value >= 0.6:
                                    color = (0, 255, 0)  # Green for close
                                    zone_text = "CLOSE"
                                elif depth_value >= 0.4:
                                    color = (0, 255, 255)  # Yellow for medium
                                    zone_text = "MID"
                                else:
                                    color = (255, 0, 255)  # Magenta for far
                                    zone_text = "FAR"
                                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), color, 2)
                                cv2.putText(frame, f'ID:{clean_id}', (int(x1), int(y1) - 25),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, 2)
                                cv2.putText(frame, zone_text, (int(x1), int(y1) - 10),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)
                    cv2.putText(frame, f"Dubs Comprehensive V1.0 | Method: {depth_method_used}",
                                (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
                    total_text = f"TOTAL COUNT: {final_count}"
                    cv2.putText(frame, total_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)
                    writer.write(frame)
                if cap:
                    cap.release()
                writer.release()

                # Try to create web-optimized version, fallback to original if it fails
                final_output_path = tmp_out.name
                with tempfile.NamedTemporaryFile(suffix='_web.mp4', delete=False) as web_tmp:
                    web_tmp_path = web_tmp.name

                logger.info(f"ðŸ”„ Attempting ffmpeg conversion: {tmp_out.name} -> {web_tmp_path}")
                if convert_to_web_mp4(tmp_out.name, web_tmp_path):
                    logger.info(f"âœ… Web-optimized video created successfully")
                    # Use web-optimized version and clean up original
                    final_output_path = web_tmp_path
                    try:
                        os.remove(tmp_out.name)  # Clean up original temp file
                    except Exception as e:
                        logger.warning(f"Failed to clean up original temp file: {e}")
                else:
                    logger.warning(f"âš ï¸ ffmpeg conversion failed, using original file")
                    # Use original file and clean up failed web version
                    try:
                        os.remove(web_tmp_path)  # Clean up failed web conversion
                    except Exception as e:
                        logger.warning(f"Failed to clean up failed web conversion: {e}")
                processing_time = time.time() - start_time
                return {
                    'status': 'completed',
                    'job_type': 'people_count',
                    'output_image': None,
                    'output_video': final_output_path,
                    'data': {
                        'person_count': final_count,
                        'raw_track_count': len(track_history_for_merge),
                        'depth_method': depth_method_used,
                        'fps': fps,
                        'detection_strategies': list(detection_strategies_used),
                        'alerts': alerts
                    },
                    'meta': {
                        'timestamp': timezone.now().isoformat(),
                        'processing_time_seconds': processing_time,
                        'fps': fps,
                        'frame_count': frame_count
                    },
                    'error': None
                }
        except Exception as e:
            logger.exception(f"Video processing failed: {str(e)}")
            return {
                'status': 'failed',
                'job_type': 'people_count',
                'output_image': None,
                'output_video': None,
                'data': {'alerts': [], 'error': str(e)},
                'meta': {'timestamp': timezone.now().isoformat(), 'processing_time_seconds': time.time() - start_time},
                'error': {'message': str(e), 'code': 'PROCESSING_ERROR'}
            }
        finally:
            # Ensure all resources are properly cleaned up
            try:
                if 'cap' in locals() and cap:
                    cap.release()
            except Exception as e:
                logger.warning(f"Failed to release video capture: {e}")

            try:
                if 'writer' in locals() and writer:
                    writer.release()
            except Exception as e:
                logger.warning(f"Failed to release video writer: {e}")

            try:
                if 'tmp_path' in locals() and os.path.exists(tmp_path):
                    os.remove(tmp_path)
            except Exception as e:
                logger.warning(f"Failed to remove temporary file {tmp_path}: {e}")

            # Note: final_output_path is not cleaned up here as tasks.py needs it
            # tasks.py will handle cleanup after saving to Django storage
            try:
                # Only clean up tmp_out if it's not the final_output_path
                if ('tmp_out' in locals() and 'final_output_path' in locals() and
                    tmp_out.name != final_output_path and os.path.exists(tmp_out.name)):
                    os.remove(tmp_out.name)
            except Exception as e:
                logger.warning(f"Failed to remove temporary output file {tmp_out.name}: {e}")

# ======================================
# Celery Integration
# ======================================

def tracking_video(input_path: str, output_path: str, job_id: str = None) -> Dict:
    """
    Analytics function for people counting.

    Args:
        input_path: Path to input video or image sequence
        output_path: Path to save output video
        job_id: VideoJob ID

    Returns:
        Standardized response dictionary
    """
    start_time = time.time()
    logger.info(f"ðŸš€ Starting people count job {job_id}")
    logger.info(f"ðŸ“¥ Input path: {input_path}")
    logger.info(f"ðŸ“¤ Output path: {output_path}")

    ext = os.path.splitext(input_path)[1].lower()
    image_exts = ['.jpg', '.jpeg', '.png']

    # Get total frames for accurate progress tracking
    total_frames = 1  # Default for images
    if ext not in image_exts:
        try:
            with default_storage.open(input_path, 'rb') as f:
                with tempfile.NamedTemporaryFile(suffix='.mp4', delete=False) as tmp:
                    tmp.write(f.read())
                    cap = cv2.VideoCapture(tmp.name)
                    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
                    cap.release()
                    os.remove(tmp.name)
        except Exception as e:
            logger.warning(f"Could not get frame count: {e}. Using estimate.")
            total_frames = 100  # Fallback estimate

    # Initialize progress logger with accurate frame count
    progress_logger = create_progress_logger(
        job_id=str(job_id) if job_id else "unknown",
        total_items=total_frames,
        job_type="people_count",
        logger_name="dubs_people_counting_comprehensive"
    )

    if ext in image_exts:
        progress_logger.update_progress(0, status="Processing image...", force_log=True)
        result = DubsComprehensivePeopleCounting(device='auto').process_image(input_path)
        progress_logger.update_progress(1, status="Analysis completed", force_log=True)
        progress_logger.log_completion(1)
    else:
        # FIX: Rename ProgressTracker to ProgressMonitor to avoid naming conflict
        class ProgressMonitor:
            def __init__(self, logger, total_frames):
                self.logger = logger
                self.total_frames = total_frames
                self.last_update_time = time.time()
                self.start_time = time.time()
                self.processed_frames = 0

            def update(self, frame_num, status=None):
                self.processed_frames = frame_num
                current_time = time.time()
                if current_time - self.last_update_time >= 1.0:  # Update every second
                    elapsed = current_time - self.start_time
                    fps = frame_num / elapsed if elapsed > 0 else 0
                    remaining = (self.total_frames - frame_num) / fps if fps > 0 else 0
                    eta = f"{int(remaining // 60):02d}:{int(remaining % 60):02d}"
                    progress = (frame_num / self.total_frames) * 100
                    status_msg = f"Processing frame {frame_num}/{self.total_frames} ({fps:.1f} FPS, ETA: {eta})"
                    self.logger.update_progress(frame_num, status=status_msg, force_log=True)
                    self.last_update_time = current_time

        monitor = ProgressMonitor(progress_logger, total_frames)
        progress_logger.update_progress(0, status="Starting video processing...", force_log=True)

        # Create a subclass of DubsComprehensivePeopleCounting to track progress
        class ProgressDubsComprehensivePeopleCounting(DubsComprehensivePeopleCounting):
            def __init__(self, progress_monitor, *args, **kwargs):
                super().__init__(*args, **kwargs)
                # FIX: Use self.progress_monitor instead of self.tracker to avoid conflict
                self.progress_monitor = progress_monitor

            def process_video(self, video_path: str, output_path: str, job_id: str = None) -> Dict:
                # Override process_video to track progress
                result = super().process_video(video_path, output_path, job_id)
                # Update progress based on actual frames processed
                if 'frame_count' in result.get('meta', {}):
                    self.progress_monitor.update(result['meta']['frame_count'])
                return result

        # Process video with progress tracking
        counter = ProgressDubsComprehensivePeopleCounting(monitor, device='auto')
        result = counter.process_video(input_path, output_path, job_id)

        # Ensure we mark as complete
        progress_logger.update_progress(total_frames, status="Video processing completed", force_log=True)
        progress_logger.log_completion(total_frames)

    processing_time = time.time() - start_time
    result['meta']['processing_time_seconds'] = processing_time
    result['meta']['timestamp'] = timezone.now().isoformat()

    return result

# ======================================
# Helper Functions
# ======================================

def validate_input_file(file_path: str) -> tuple[bool, str]:
    """Validate file type and size."""
    if not default_storage.exists(file_path):
        return False, f"File not found: {file_path}"
    ext = os.path.splitext(file_path)[1].lower()
    if ext not in VALID_EXTENSIONS:
        return False, f"Invalid file type: {ext}. Allowed: {', '.join(VALID_EXTENSIONS)}"
    size = default_storage.size(file_path)
    if size > MAX_FILE_SIZE:
        return False, f"File size {size / (1024*1024):.2f}MB exceeds 500MB limit"
    return True, ""
